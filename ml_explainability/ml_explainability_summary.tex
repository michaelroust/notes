\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{geometry}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{amsthm}
\usepackage{mathtools}

\geometry{margin=1in}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{example}{Example}[section]
\newtheorem{proposition}{Proposition}[section]

\title{ML Explainability Summary}
\author{Mathematical Notes}
\date{\today}

\begin{document}

\maketitle

\tableofcontents
\newpage

\section{Introduction to Explainability}

\subsection{Motivation for Explainability}
\begin{itemize}
    \item \textbf{Trust}: Users need to understand model decisions
    \item \textbf{Fairness}: Detect and mitigate bias
    \item \textbf{Debugging}: Identify model failures
    \item \textbf{Compliance}: Regulatory requirements (GDPR, AI Act)
    \item \textbf{Scientific understanding}: Gain insights from data
    \item \textbf{Model improvement}: Identify areas for enhancement
\end{itemize}

\subsection{Types of Explainability}
\begin{definition}
\textbf{Global explainability} provides understanding of the overall model behavior and decision-making process.
\end{definition}

\begin{definition}
\textbf{Local explainability} explains individual predictions for specific instances.
\end{definition}

\begin{definition}
\textbf{Post-hoc explainability} generates explanations after model training, without modifying the model.
\end{definition}

\begin{definition}
\textbf{Intrinsic explainability} uses inherently interpretable models like linear regression or decision trees.
\end{definition}

\subsection{Properties of Good Explanations}
\begin{itemize}
    \item \textbf{Faithfulness}: How well does the explanation reflect the actual model behavior?
    \item \textbf{Stability}: Are explanations consistent across similar inputs?
    \item \textbf{Completeness}: Does the explanation capture all relevant factors?
    \item \textbf{Simplicity}: Is the explanation easy to understand?
    \item \textbf{Contrastiveness}: Does it explain why this prediction rather than alternatives?
\end{itemize}

\section{Feature Importance Methods}

\subsection{Permutation Importance}
\begin{definition}
\textbf{Permutation importance} measures the increase in prediction error when a feature is randomly shuffled, breaking its relationship with the target.
\end{definition}

For feature $j$:
$$\text{PI}_j = s - \frac{1}{K}\sum_{k=1}^K s_{k,j}$$
where $s$ is the baseline score and $s_{k,j}$ is the score when feature $j$ is permuted in permutation $k$.

\subsection{SHAP Values}
\begin{definition}
\textbf{SHAP (SHapley Additive exPlanations)} values satisfy the efficiency, symmetry, dummy, and additivity axioms:
$$\phi_i(f, \mathbf{x}) = \sum_{S \subseteq F \setminus \{i\}} \frac{|S|!(|F|-|S|-1)!}{|F|!}[f_{S \cup \{i\}}(\mathbf{x}_{S \cup \{i\}}) - f_S(\mathbf{x}_S)]$$
where $F$ is the set of all features.
\end{definition}

\subsection{Integrated Gradients}
\begin{definition}
\textbf{Integrated Gradients} computes the integral of gradients along the path from baseline to input:
$$\text{IG}_i(\mathbf{x}) = (x_i - x_i')\int_{\alpha=0}^1 \frac{\partial f(\mathbf{x}' + \alpha(\mathbf{x} - \mathbf{x}'))}{\partial x_i} d\alpha$$
where $\mathbf{x}'$ is the baseline input.
\end{definition}

\subsection{Layer-wise Relevance Propagation (LRP)}
\begin{definition}
\textbf{LRP} propagates relevance scores backward through neural network layers to identify important input features.
\end{definition}

\section{Local Explainability Methods}

\subsection{LIME (Local Interpretable Model-agnostic Explanations)}
\begin{definition}
\textbf{LIME} approximates the model locally around a prediction using a simple interpretable model:
$$\xi(x) = \arg\min_{g \in G} \mathcal{L}(f, g, \pi_x) + \Omega(g)$$
where $G$ is a class of interpretable models, $\pi_x$ is a proximity measure, and $\Omega(g)$ penalizes complexity.
\end{definition}

\subsection{SHAP Local Explanations}
\begin{definition}
\textbf{SHAP local explanations} provide feature attributions for individual predictions using the Shapley value framework.
\end{definition}

\subsection{Anchors}
\begin{definition}
\textbf{Anchors} find the minimal set of features that, when present, guarantee a specific prediction with high confidence.
\end{definition}

\section{Gradient-Based Methods}

\subsection{Grad-CAM}
\begin{definition}
\textbf{Grad-CAM} generates visual explanations by computing gradients of the target class with respect to feature maps:
$$L_{Grad-CAM}^c = \text{ReLU}\left(\sum_k \alpha_k^c A^k\right)$$
where $\alpha_k^c = \frac{1}{Z}\sum_i \frac{\partial y^c}{\partial A_{ij}^k}$ and $A^k$ are the feature maps.
\end{definition}

\subsection{Guided Grad-CAM}
\begin{definition}
\textbf{Guided Grad-CAM} combines Grad-CAM with guided backpropagation for finer-grained visualizations.
\end{definition}

\subsection{SmoothGrad}
\begin{definition}
\textbf{SmoothGrad} reduces noise in gradient-based explanations by averaging gradients over multiple noisy versions of the input.
\end{definition}

\subsection{Integrated Gradients}
\begin{definition}
\textbf{Integrated Gradients} satisfies the sensitivity and implementation invariance axioms for attribution methods.
\end{definition}

\section{Attention-Based Explanations}

\subsection{Attention Visualization}
\begin{definition}
\textbf{Attention weights} in transformer models can be visualized to show which input tokens the model focuses on for each prediction.
\end{definition}

\subsection{Attention Rollout}
\begin{definition}
\textbf{Attention rollout} aggregates attention weights across all layers to understand the flow of information.
\end{definition}

\subsection{Attention Flow}
\begin{definition}
\textbf{Attention flow} traces how information flows through attention mechanisms to identify important input regions.
\end{definition}

\section{Surrogate Models}

\subsection{Global Surrogate Models}
\begin{definition}
A \textbf{global surrogate model} is a simpler, interpretable model trained to mimic the behavior of a complex model across the entire input space.
\end{definition}

\subsection{Local Surrogate Models}
\begin{definition}
A \textbf{local surrogate model} approximates the complex model's behavior in a specific region around a given input.
\end{definition}

\subsection{Decision Trees as Surrogates}
\begin{definition}
\textbf{Decision trees} can serve as surrogate models by learning rules that approximate the complex model's decision boundaries.
\end{definition}

\section{Interpretable Models}

\subsection{Linear Models}
\begin{definition}
\textbf{Linear models} are inherently interpretable as coefficients directly indicate feature importance and direction of influence.
\end{definition}

\subsection{Generalized Additive Models (GAMs)}
\begin{definition}
\textbf{GAMs} model the target as a sum of smooth functions of individual features:
$$g(\mathbb{E}[Y]) = \beta_0 + f_1(x_1) + f_2(x_2) + \cdots + f_p(x_p)$$
\end{definition}

\subsection{Decision Trees}
\begin{definition}
\textbf{Decision trees} provide interpretable rules through recursive binary splits based on feature thresholds.
\end{definition}

\subsection{Rule-Based Models}
\begin{definition}
\textbf{Rule-based models} express predictions as logical rules that are easy to understand and validate.
\end{definition}

\section{Counterfactual Explanations}

\subsection{Counterfactual Generation}
\begin{definition}
A \textbf{counterfactual explanation} answers: "What would need to change in the input to get a different prediction?"
\end{definition}

\subsection{Minimal Changes}
\begin{definition}
\textbf{Minimal counterfactuals} find the smallest changes to the input that would result in a different prediction.
\end{definition}

\subsection{Actionable Counterfactuals}
\begin{definition}
\textbf{Actionable counterfactuals} consider only changes that are feasible in the real world.
\end{definition}

\section{Causal Explainability}

\subsection{Causal Inference}
\begin{definition}
\textbf{Causal inference} methods aim to understand cause-and-effect relationships rather than just correlations.
\end{definition}

\subsection{Interventional Explanations}
\begin{definition}
\textbf{Interventional explanations} show how changing specific features would affect the prediction, accounting for causal relationships.
\end{definition}

\subsection{Causal Discovery}
\begin{definition}
\textbf{Causal discovery} algorithms learn causal graphs from observational data to understand feature relationships.
\end{definition}

\section{Concept-Based Explanations}

\subsection{Concept Activation Vectors (CAVs)}
\begin{definition}
\textbf{CAVs} represent human-interpretable concepts as directions in the neural network's activation space.
\end{definition}

\subsection{Testing with Concept Activation Vectors (TCAV)}
\begin{definition}
\textbf{TCAV} quantifies the influence of concepts on model predictions using CAVs.
\end{definition}

\subsection{Concept Bottleneck Models}
\begin{definition}
\textbf{Concept bottleneck models} explicitly model the relationship between input features and human-interpretable concepts.
\end{definition}

\section{Evaluation of Explanations}

\subsection{Faithfulness Metrics}
\begin{itemize}
    \item \textbf{Deletion}: Remove important features and measure performance drop
    \item \textbf{Insertion}: Add important features and measure performance gain
    \item \textbf{ROAR}: Remove and retrain to evaluate explanation quality
\end{itemize}

\subsection{Stability Metrics}
\begin{itemize}
    \item \textbf{Consistency}: Similar inputs should have similar explanations
    \item \textbf{Continuity}: Small input changes should not cause large explanation changes
\end{itemize}

\subsection{Human Evaluation}
\begin{itemize}
    \item \textbf{Comprehensibility}: How well do humans understand the explanation?
    \item \textbf{Trustworthiness}: Do explanations increase user trust?
    \item \textbf{Actionability}: Can users act on the explanations?
\end{itemize}

\section{Challenges and Limitations}

\subsection{Adversarial Explanations}
\begin{definition}
\textbf{Adversarial explanations} can be manipulated to hide model biases or create misleading interpretations.
\end{definition}

\subsection{Computational Complexity}
Many explainability methods are computationally expensive, especially for large models and datasets.

\subsection{Evaluation Challenges}
\begin{itemize}
    \item Lack of ground truth for explanations
    \item Difficulty in measuring explanation quality
    \item Subjectivity in human evaluation
\end{itemize}

\subsection{Model-Specific Limitations}
\begin{itemize}
    \item Some methods only work with specific model types
    \item Gradient-based methods require differentiable models
    \item Attention-based methods only apply to attention-based architectures
\end{itemize}

\section{Applications}

\subsection{Healthcare}
\begin{itemize}
    \item Medical diagnosis explanations
    \item Treatment recommendation reasoning
    \item Drug discovery insights
    \item Clinical decision support
\end{itemize}

\subsection{Finance}
\begin{itemize}
    \item Credit scoring explanations
    \item Fraud detection reasoning
    \item Risk assessment transparency
    \item Regulatory compliance
\end{itemize}

\subsection{Autonomous Systems}
\begin{itemize}
    \item Self-driving car decision explanations
    \item Robot behavior understanding
    \item Safety-critical system validation
\end{itemize}

\subsection{Legal and Compliance}
\begin{itemize}
    \item Algorithmic decision explanations
    \item Bias detection and mitigation
    \item Audit trail generation
    \item Regulatory reporting
\end{itemize}

\section{Ethical Considerations}

\subsection{Fairness and Bias}
\begin{itemize}
    \item Detecting algorithmic bias
    \item Ensuring fair explanations across groups
    \item Mitigating discriminatory practices
\end{itemize}

\subsection{Privacy}
\begin{itemize}
    \item Protecting sensitive information in explanations
    \item Differential privacy in explanation generation
    \item Avoiding data leakage through explanations
\end{itemize}

\subsection{Transparency vs. Security}
\begin{itemize}
    \item Balancing transparency with model security
    \item Preventing adversarial attacks through explanations
    \item Protecting intellectual property
\end{itemize}

\section{Future Directions}

\subsection{Interactive Explanations}
\begin{itemize}
    \item Dialogue-based explanation systems
    \item User-guided explanation generation
    \item Iterative refinement of explanations
\end{itemize}

\subsection{Multi-Modal Explanations}
\begin{itemize}
    \item Combining text, visual, and numerical explanations
    \item Cross-modal explanation consistency
    \item Personalized explanation formats
\end{itemize}

\subsection{Automated Explanation Generation}
\begin{itemize}
    \item Natural language explanation generation
    \item Automated explanation quality assessment
    \item Self-explaining models
\end{itemize}

\subsection{Regulatory Compliance}
\begin{itemize}
    \item Standardized explanation formats
    \item Automated compliance checking
    \item Industry best practices
\end{itemize}

\section{Important Algorithms}

\subsection{Explanation Generation}
\begin{itemize}
    \item SHAP
    \item LIME
    \item Grad-CAM
    \item Integrated Gradients
    \item Permutation importance
    \item Counterfactual generation
\end{itemize}

\subsection{Evaluation Methods}
\begin{itemize}
    \item Faithfulness testing
    \item Stability analysis
    \item Human evaluation protocols
    \item Automated quality metrics
\end{itemize}

\section{Key Theorems}

\subsection{Shapley Value Properties}
\begin{theorem}
The Shapley value is the unique solution that satisfies efficiency, symmetry, dummy, and additivity axioms.
\end{theorem}

\subsection{Explanation Completeness}
\begin{theorem}
For any explanation method that satisfies the efficiency axiom, the sum of feature attributions equals the difference between the prediction and the baseline.
\end{theorem}

\subsection{Uniqueness of Integrated Gradients}
\begin{theorem}
Integrated Gradients is the unique attribution method that satisfies sensitivity, implementation invariance, and completeness axioms.
\end{theorem}

\end{document}
